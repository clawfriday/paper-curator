# =============================================================================
# Server ports for HPC deployment
# =============================================================================
server:
  frontend_port: 3000
  backend_port: 3100

# =============================================================================
# Ingestion settings
# =============================================================================
ingestion:
  skip_existing: true  # Set to true to skip papers already in database during batch ingest

# =============================================================================
# Slack channels for paper ingestion
# =============================================================================
slack:
  # Add Slack channel URLs here (one per line)
  # Format: https://app.slack.com/client/WORKSPACE_ID/CHANNEL_ID
  channels:
    - https://app.slack.com/client/T04MW5HMWV9/C0A727EKAJV
    - https://app.slack.com/client/T04MW5HMWV9/C04SVKLPERH

# =============================================================================
# OpenAI-compatible API endpoints
# NOTE: You can use localhost URLs - they auto-convert to host.docker.internal in Docker
# =============================================================================
endpoints:
  llm_base_url: "http://localhost:8001/v1"  # LLM requests go here (affects all LLM features)
  embedding_base_url: "http://localhost:8004/v1"  # Embedding requests go here (affects clustering + similarity)
  api_key: "local-key"  # Shared key for the endpoints above

# =============================================================================
# PaperQA2 settings
# =============================================================================
paperqa:
  chunk_chars: 5000  # Larger chunks reduce index size but may blur details
  chunk_overlap: 250  # More overlap improves recall but increases compute
  use_doc_details: false  # Adds metadata into prompts for better grounding
  evidence_k: 10  # More evidence increases accuracy but adds latency
  evidence_summary_length: "about 100 words"  # Longer summaries add detail but cost tokens
  evidence_skip_summary: false  # Skip evidence summaries for speed if true
  evidence_relevance_score_cutoff: 1  # Higher cutoff is stricter, fewer sources used

# =============================================================================
# UI behavior settings
# =============================================================================
ui:
  hover_debounce_ms: 500  # Higher = fewer hover LLM calls, lower = more responsive
  max_similar_papers: 5  # Higher = more results, more API usage
  tree_auto_save_interval_ms: 30000  # Higher = fewer writes, lower = less data loss
  tree_category_font_size: 20  # Font size for category nodes in tree diagram
  tree_paper_font_size: 20  # Font size for paper nodes in tree diagram

# =============================================================================
# Classification settings
# =============================================================================
classification:
  # Max k for k-means clustering (2, 3, ..., branching_factor)
  # Higher = more children per node, wider tree
  branching_factor: 5  # Higher = wider tree, fewer levels, more children per node
  # Auto-rebuild tree after ingesting new papers
  rebuild_on_ingest: true  # If false, tree updates only on manual re-classify

# =============================================================================
# Topic Query settings (multi-paper RAG)
# =============================================================================
topic_query:
  max_papers_per_batch: 10       # Papers shown per batch in paper selector
  similarity_threshold: 0.5     # Minimum cosine similarity (0-1) for paper matching
  chunks_per_paper: 5           # Top chunks retrieved per paper during query
  debug_mode: false             # Save intermediate prompts to storage/schemas/topic_query.json

# =============================================================================
# External API settings
# =============================================================================
external_apis:
  papers_with_code_enabled: true  # Enable repo lookup via Papers With Code
  github_search_enabled: true  # Enable repo lookup via GitHub search
  semantic_scholar_enabled: true  # Enable references and similar papers
  # Optional: GitHub token for higher rate limits
  # github_token: "ghp_xxxxx"
  # Optional: Semantic Scholar API key for higher rate limits
  # Get one free at: https://www.semanticscholar.org/product/api#api-key-form
  # semantic_scholar_api_key: "xxx"
