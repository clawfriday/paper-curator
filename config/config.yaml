# =============================================================================
# OpenAI-compatible API endpoints
# NOTE: You can use localhost URLs - they auto-convert to host.docker.internal in Docker
# =============================================================================
endpoints:
  llm_base_url: "https://transmissively-conidial-fredrick.ngrok-free.dev/v1"  # LLM requests go here (affects all LLM features)
  embedding_base_url: "http://localhost:8004/v1"  # Embedding requests go here (affects clustering + similarity)
  api_key: "local-key"  # Shared key for the endpoints above

# =============================================================================
# PaperQA2 settings
# =============================================================================
paperqa:
  chunk_chars: 5000  # Larger chunks reduce index size but may blur details
  chunk_overlap: 250  # More overlap improves recall but increases compute
  use_doc_details: true  # Adds metadata into prompts for better grounding
  evidence_k: 10  # More evidence increases accuracy but adds latency
  evidence_summary_length: "about 100 words"  # Longer summaries add detail but cost tokens
  evidence_skip_summary: false  # Skip evidence summaries for speed if true
  evidence_relevance_score_cutoff: 1  # Higher cutoff is stricter, fewer sources used

# =============================================================================
# UI behavior settings
# =============================================================================
ui:
  hover_debounce_ms: 500  # Higher = fewer hover LLM calls, lower = more responsive
  max_similar_papers: 5  # Higher = more results, more API usage
  tree_auto_save_interval_ms: 30000  # Higher = fewer writes, lower = less data loss

# =============================================================================
# Classification settings
# =============================================================================
classification:
  # Max k for k-means clustering (2, 3, ..., branching_factor)
  # Higher = more children per node, wider tree
  branching_factor: 5  # Higher = wider tree, fewer levels, more children per node
  # Auto-rebuild tree after ingesting new papers
  rebuild_on_ingest: true  # If false, tree updates only on manual re-classify

# =============================================================================
# External API settings
# =============================================================================
external_apis:
  papers_with_code_enabled: true  # Enable repo lookup via Papers With Code
  github_search_enabled: true  # Enable repo lookup via GitHub search
  semantic_scholar_enabled: true  # Enable references and similar papers
  # Optional: GitHub token for higher rate limits
  # github_token: "ghp_xxxxx"
  # Optional: Semantic Scholar API key for higher rate limits
  # Get one free at: https://www.semanticscholar.org/product/api#api-key-form
  # semantic_scholar_api_key: "xxx"
